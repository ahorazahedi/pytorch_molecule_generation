{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import HDFDataset\n",
    "dataset    = HDFDataset('./data/train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_i, edges_i, apd_i = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 8]), torch.Size([13, 13, 4]), torch.Size([833]))"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_i.shape, edges_i.shape, apd_i.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Parameters import Parameters as C\n",
    "\n",
    "BIG_POSITIVE = C.big_positive\n",
    "\n",
    "TORCH_DEVICE = C.device\n",
    "class MLP(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_layer_sizes, out_features, init, dropout_p , activation_function = torch.nn.SELU):\n",
    "        super(MLP, self).__init__()\n",
    "    \n",
    "        fs = [in_features, *hidden_layer_sizes, out_features]\n",
    "        layers = [self.generate_block(input_linear, out_linear,\n",
    "                                     activation_function, init,\n",
    "                                     dropout_p)\n",
    "                  for input_linear, out_linear in zip(fs, fs[1:])]\n",
    "        layers = [module for sq in layers for module in sq.children()]\n",
    "        self.sequence_of_layers = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def generate_block(self, in_f, out_f, activation, init, dropout_p):\n",
    "        linear = torch.nn.Linear(in_f, out_f, bias=True)\n",
    "        return torch.nn.Sequential(linear, activation(), torch.nn.AlphaDropout(dropout_p))\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        return self.sequence_of_layers(input)\n",
    "\n",
    "class GlobalReadout(torch.nn.Module):\n",
    "   \n",
    "    def __init__(self, f_add_elems, f_conn_elems, f_term_elems, mlp1_depth,\n",
    "                 mlp1_dropout_p, mlp1_hidden_dim, mlp2_depth, mlp2_dropout_p,\n",
    "                 mlp2_hidden_dim, graph_emb_size, init, max_n_nodes, node_emb_size):\n",
    "\n",
    "        super(GlobalReadout, self).__init__()\n",
    "\n",
    "        self.fAddNet1 = MLP(\n",
    "            in_features=node_emb_size,\n",
    "            hidden_layer_sizes=[mlp1_hidden_dim] * mlp1_depth,\n",
    "            out_features=f_add_elems,\n",
    "            init=init,\n",
    "            dropout_p=mlp1_dropout_p , \n",
    "            activation_function=torch.nn.ReLU\n",
    "        )\n",
    "\n",
    "        self.fConnNet1 = MLP(\n",
    "            in_features=node_emb_size,\n",
    "            hidden_layer_sizes=[mlp1_hidden_dim] * mlp1_depth,\n",
    "            out_features=f_conn_elems,\n",
    "            init=init,\n",
    "            dropout_p=mlp1_dropout_p , \n",
    "            activation_function=torch.nn.ReLU\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.fAddNet2 = MLP(\n",
    "            in_features=(max_n_nodes * f_add_elems + graph_emb_size),\n",
    "            hidden_layer_sizes=[mlp2_hidden_dim] * mlp2_depth,\n",
    "            out_features=f_add_elems * max_n_nodes,\n",
    "            init=init,\n",
    "            dropout_p=mlp2_dropout_p , \n",
    "            activation_function=torch.nn.ReLU\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.fConnNet2 = MLP(\n",
    "            in_features=(max_n_nodes * f_conn_elems + graph_emb_size),\n",
    "            hidden_layer_sizes=[mlp2_hidden_dim] * mlp2_depth,\n",
    "            out_features=f_conn_elems * max_n_nodes,\n",
    "            init=init,\n",
    "            dropout_p=mlp2_dropout_p , \n",
    "            activation_function=torch.nn.ReLU\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.fTermNet2 = MLP(\n",
    "            in_features=graph_emb_size,\n",
    "            hidden_layer_sizes=[mlp2_hidden_dim] * mlp2_depth,\n",
    "            out_features=f_term_elems,\n",
    "            init=init,\n",
    "            dropout_p=mlp2_dropout_p , \n",
    "            activation_function=torch.nn.ReLU\n",
    "        )\n",
    "\n",
    "    def forward(self, node_level_output, graph_embedding_batch):\n",
    "\n",
    "\n",
    "        print(\"node_level_output\" , node_level_output.shape)\n",
    "        self.fAddNet1 = self.fAddNet1.to(TORCH_DEVICE, non_blocking=True)\n",
    "        self.fConnNet1 = self.fConnNet1.to(TORCH_DEVICE, non_blocking=True)\n",
    "        self.fAddNet2 = self.fAddNet2.to(TORCH_DEVICE, non_blocking=True)\n",
    "        self.fConnNet2 = self.fConnNet2.to(TORCH_DEVICE, non_blocking=True)\n",
    "        self.fTermNet2 = self.fTermNet2.to(TORCH_DEVICE, non_blocking=True)\n",
    "\n",
    "        f_add_1 = self.fAddNet1(node_level_output)\n",
    "        f_conn_1 = self.fConnNet1(node_level_output)\n",
    "\n",
    "        f_add_1 = f_add_1.to(TORCH_DEVICE, non_blocking=True)\n",
    "        f_conn_1 = f_conn_1.to(TORCH_DEVICE, non_blocking=True)\n",
    "\n",
    "    \n",
    "        f_add_1_size = f_add_1.size()\n",
    "        f_conn_1_size = f_conn_1.size()\n",
    "        \n",
    "        # print(\"f_add_1\" ,f_add_1.shape)\n",
    "        f_add_1 = f_add_1.view((f_add_1_size[0], f_add_1_size[1] * f_add_1_size[2]))\n",
    "        # print(\"f_add_1\" ,f_add_1.shape)\n",
    "        \n",
    "        f_conn_1 = f_conn_1.view((f_conn_1_size[0], f_conn_1_size[1] * f_conn_1_size[2]))\n",
    "        # print(\"graph_embedding_batch\" , graph_embedding_batch.shape)\n",
    "        # print(\"f_2_add_input\" , torch.cat((f_add_1, graph_embedding_batch), dim=1).unsqueeze(dim=1).shape)\n",
    "        f_add_2 = self.fAddNet2(torch.cat((f_add_1, graph_embedding_batch), dim=1).unsqueeze(dim=1))\n",
    "        f_conn_2 = self.fConnNet2(torch.cat((f_conn_1, graph_embedding_batch), dim=1).unsqueeze(dim=1))\n",
    "        f_term_2 = self.fTermNet2(graph_embedding_batch)\n",
    "\n",
    "        f_add_2 = f_add_2.to(TORCH_DEVICE)\n",
    "        f_conn_2 = f_conn_2.to(TORCH_DEVICE)\n",
    "        f_term_2 = f_term_2.to(TORCH_DEVICE)\n",
    "\n",
    "        cat = torch.cat((f_add_2.squeeze(dim=1), f_conn_2.squeeze(dim=1), f_term_2), dim=1)\n",
    "\n",
    "        return cat  \n",
    "\n",
    "class GraphGather(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, node_features, hidden_node_features, out_features,\n",
    "                 att_depth, att_hidden_dim, att_dropout_p, emb_depth,\n",
    "                 emb_hidden_dim, emb_dropout_p, init):\n",
    "\n",
    "        super(GraphGather, self).__init__()\n",
    "\n",
    "        self.att_nn = MLP(\n",
    "            in_features=node_features + hidden_node_features,\n",
    "            hidden_layer_sizes=[att_hidden_dim] * att_depth,\n",
    "            out_features=out_features,\n",
    "            init=init,\n",
    "            dropout_p=att_dropout_p\n",
    "        )\n",
    "\n",
    "        self.emb_nn = MLP(\n",
    "            in_features=hidden_node_features,\n",
    "            hidden_layer_sizes=[emb_hidden_dim] * emb_depth,\n",
    "            out_features=out_features,\n",
    "            init=init,\n",
    "            dropout_p=emb_dropout_p\n",
    "        )\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, hidden_nodes, input_nodes, node_mask):\n",
    "\n",
    "        cat = torch.cat((hidden_nodes, input_nodes), dim=2)\n",
    "        energy_mask = (node_mask == 0).float() * BIG_POSITIVE\n",
    "        energies = self.att_nn(cat) - energy_mask.unsqueeze(-1)\n",
    "        attention = self.softmax(energies)\n",
    "        embedding = self.emb_nn(hidden_nodes)\n",
    "\n",
    "        return torch.sum(attention * embedding, dim=1)\n",
    "\n",
    "class SummationMPNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, hidden_node_features, edge_features, message_size, message_passes):\n",
    "\n",
    "        super(SummationMPNN, self).__init__()\n",
    "\n",
    "        self.node_features = node_features\n",
    "        self.hidden_node_features = hidden_node_features\n",
    "        self.edge_features = edge_features\n",
    "        self.message_size = message_size\n",
    "        self.message_passes = message_passes\n",
    "\n",
    "    def message_terms(self, nodes, node_neighbours, edges):\n",
    "      # Message Passing Function is Here \n",
    "      # nodes Shape [batch , node_features]\n",
    "      # node_nb Shape [batch, max node , number of node features]\n",
    "      # edges [batch max_node edge feature]\n",
    "      \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, nodes, messages):\n",
    "        \n",
    "      # Message update function is Here \n",
    "      # nodes Shape [batch , node_features]\n",
    "      # messages Shape [batch , number of node features]\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def graph_readout(self, hidden_nodes, input_nodes, node_mask):\n",
    "\n",
    "        #Local readout function\n",
    "        # hidden_nodes shape [batch , node_feat]\n",
    "        # input_nodes shape [batch , node_feat]\n",
    "        # node_mask shape [batch , node_feat]\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def apd_calc(self , graph_embedding , hidden_nodes ):\n",
    "    \n",
    "        raise NotImplemented\n",
    "    def forward(self, nodes, edges):\n",
    "   \n",
    "        # nodes shape [number of subgraphs , node features , number of nodes]\n",
    "        # edges shape [number of subgraphs , number_of_node , number_of_node number_of_edge_feat]\n",
    "\n",
    "        adjacency = torch.sum(edges, dim=3)\n",
    "        \n",
    "        # This Line Create 3 tensor\n",
    "        # first one is like [0,0,0,0,1,1,1,1] indicate that each edge belongs to each item in this batch\n",
    "        #secound one is in item node id of src and third one is destination of that edge\n",
    "        #for example Node 1 is connected to node 2 in item batch of 1\n",
    "        \n",
    "        (\n",
    "            edge_batch_batch_idc,\n",
    "            edge_batch_node_idc,\n",
    "            edge_batch_nghb_idc,\n",
    "        ) = adjacency.nonzero(as_tuple=True)\n",
    "    \n",
    "\n",
    "        (node_batch_batch_idc, node_batch_node_idc) = adjacency.sum(-1).nonzero(as_tuple=True)\n",
    "        \n",
    "        same_batch = node_batch_batch_idc.view(-1, 1) == edge_batch_batch_idc\n",
    "        same_node = node_batch_node_idc.view(-1, 1) == edge_batch_node_idc\n",
    "        \n",
    "        message_summation_matrix = (same_batch * same_node).float()\n",
    "\n",
    "        edge_batch_edges = edges[edge_batch_batch_idc, edge_batch_node_idc, edge_batch_nghb_idc, :]\n",
    "        \n",
    "        hidden_nodes = torch.zeros(nodes.shape[0], nodes.shape[1], self.hidden_node_features, device=TORCH_DEVICE)\n",
    "        \n",
    "        hidden_nodes[:nodes.shape[0], :nodes.shape[1], :nodes.shape[2]] = nodes.clone()\n",
    "        \n",
    "        node_batch_nodes = hidden_nodes[node_batch_batch_idc, node_batch_node_idc, :]\n",
    "\n",
    "        for _ in range(self.message_passes):\n",
    "            edge_batch_nodes = hidden_nodes[edge_batch_batch_idc, edge_batch_node_idc, :]\n",
    "\n",
    "            edge_batch_nghbs = hidden_nodes[edge_batch_batch_idc, edge_batch_nghb_idc, :]\n",
    "\n",
    "            message_terms = self.message_terms(edge_batch_nodes,\n",
    "                                               edge_batch_nghbs,\n",
    "                                               edge_batch_edges)\n",
    "\n",
    "            if len(message_terms.size()) == 1: \n",
    "                message_terms = message_terms.unsqueeze(0)\n",
    "\n",
    "\n",
    "            messages = torch.matmul(message_summation_matrix, message_terms)\n",
    "\n",
    "            node_batch_nodes = self.update(node_batch_nodes, messages)\n",
    "            \n",
    "            hidden_nodes[node_batch_batch_idc, node_batch_node_idc, :] = node_batch_nodes.clone()\n",
    "\n",
    "        node_mask = adjacency.sum(-1) != 0\n",
    "\n",
    "        graph_embedding = self.graph_readout(hidden_nodes, nodes, node_mask)\n",
    "        \n",
    "        output = self.apd_calc(graph_embedding , hidden_nodes)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GGNN(SummationMPNN):\n",
    " \n",
    "    def __init__(self, edge_features, enn_depth, enn_dropout_p, enn_hidden_dim,\n",
    "                 f_add_elems, mlp1_depth, mlp1_dropout_p, mlp1_hidden_dim,\n",
    "                 mlp2_depth, mlp2_dropout_p, mlp2_hidden_dim, gather_att_depth,\n",
    "                 gather_att_dropout_p, gather_att_hidden_dim, gather_width,\n",
    "                 gather_emb_depth, gather_emb_dropout_p, gather_emb_hidden_dim,\n",
    "                 hidden_node_features, initialization, message_passes,\n",
    "                 message_size, n_nodes_largest_graph, node_features):\n",
    "\n",
    "        super(GGNN, self).__init__(node_features, hidden_node_features, edge_features, message_size, message_passes)\n",
    "\n",
    "        self.n_nodes_largest_graph = n_nodes_largest_graph\n",
    "        \n",
    "\n",
    "        self.msg_nns = torch.nn.ModuleList()\n",
    "        for _ in range(edge_features):\n",
    "            self.msg_nns.append(\n",
    "                MLP(\n",
    "                    in_features=hidden_node_features,\n",
    "                    hidden_layer_sizes=[enn_hidden_dim] * enn_depth,\n",
    "                    out_features=message_size,\n",
    "                    init=initialization,\n",
    "                    dropout_p=enn_dropout_p,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.gru = torch.nn.GRUCell(\n",
    "            input_size=message_size, hidden_size=hidden_node_features, bias=True\n",
    "        )\n",
    "\n",
    "        self.gather = GraphGather(\n",
    "            node_features=node_features,\n",
    "            hidden_node_features=hidden_node_features,\n",
    "            out_features=gather_width,\n",
    "            att_depth=gather_att_depth,\n",
    "            att_hidden_dim=gather_att_hidden_dim,\n",
    "            att_dropout_p=gather_att_dropout_p,\n",
    "            emb_depth=gather_emb_depth,\n",
    "            emb_hidden_dim=gather_emb_hidden_dim,\n",
    "            emb_dropout_p=gather_emb_dropout_p,\n",
    "            init=initialization,\n",
    "        )\n",
    "\n",
    "        self.APDReadout = GlobalReadout(\n",
    "            node_emb_size=hidden_node_features,\n",
    "            graph_emb_size=gather_width,\n",
    "            mlp1_hidden_dim=mlp1_hidden_dim,\n",
    "            mlp1_depth=mlp1_depth,\n",
    "            mlp1_dropout_p=mlp1_dropout_p,\n",
    "            mlp2_hidden_dim=mlp2_hidden_dim,\n",
    "            mlp2_depth=mlp2_depth,\n",
    "            mlp2_dropout_p=mlp2_dropout_p,\n",
    "            init=initialization,\n",
    "            f_add_elems=f_add_elems,\n",
    "            f_conn_elems=edge_features,\n",
    "            f_term_elems=1,\n",
    "            max_n_nodes=n_nodes_largest_graph,\n",
    "        )\n",
    "\n",
    "    def message_terms(self, nodes, node_neighbours, edges):\n",
    "        \n",
    "        edges_v = edges.view(-1, self.edge_features, 1)\n",
    "        \n",
    "        node_neighbours_v = edges_v * node_neighbours.view(-1, 1, self.hidden_node_features)\n",
    "        \n",
    "        \n",
    "        terms_masked_per_edge = [\n",
    "            edges_v[:, i, :] * self.msg_nns[i](node_neighbours_v[:, i, :])\n",
    "            for i in range(self.edge_features)\n",
    "        ]\n",
    "        \n",
    "        return sum(terms_masked_per_edge)\n",
    "\n",
    "    def update(self, nodes, messages):\n",
    "        # Run This Function When Updating Node Embedding is Required\n",
    "        update_out = self.gru(messages, nodes)\n",
    "        \n",
    "        # print(\"Update messages \"  , messages.shape)\n",
    "        # print(\"Update nodes\"  , nodes.shape)\n",
    "        # print(\"Update update_out\"  , update_out.shape)\n",
    "        \n",
    "        return update_out\n",
    "\n",
    "    def graph_readout(self, hidden_nodes, input_nodes, node_mask):\n",
    "        # This Message Run Once\n",
    "        # print(\"readout\" ,  hidden_nodes.shape, input_nodes.shape, node_mask.shape)\n",
    "        graph_embeddings = self.gather(hidden_nodes, input_nodes, node_mask)\n",
    "        # print(\"readout graph_embeddings\" ,graph_embeddings.shape)\n",
    "        # output = self.APDReadout(hidden_nodes, graph_embeddings)\n",
    "        output = graph_embeddings\n",
    "        # print(\"readout output\" ,output.shape)\n",
    "        return output\n",
    "    \n",
    "    def apd_calc(self, graph_embedding, hidden_nodes):\n",
    "        output = self.APDReadout(hidden_nodes, graph_embedding)\n",
    "        return output\n",
    "      \n",
    "        \n",
    "\n",
    "def create_model():\n",
    "    net = GGNN(\n",
    "            f_add_elems=C.dim_f_add_p1,\n",
    "            edge_features=C.dim_edges[2],\n",
    "            enn_depth=C.enn_depth,\n",
    "            enn_dropout_p=C.enn_dropout_p,\n",
    "            enn_hidden_dim=C.enn_hidden_dim,\n",
    "            mlp1_depth=C.mlp1_depth,\n",
    "            mlp1_dropout_p=C.mlp1_dropout_p,\n",
    "            mlp1_hidden_dim=C.mlp1_hidden_dim,\n",
    "            mlp2_depth=C.mlp2_depth,\n",
    "            mlp2_dropout_p=C.mlp2_dropout_p,\n",
    "            mlp2_hidden_dim=C.mlp2_hidden_dim,\n",
    "            gather_att_depth=C.gather_att_depth,\n",
    "            gather_att_dropout_p=C.gather_att_dropout_p,\n",
    "            gather_att_hidden_dim=C.gather_att_hidden_dim,\n",
    "            gather_width=C.gather_width,\n",
    "            gather_emb_depth=C.gather_emb_depth,\n",
    "            gather_emb_dropout_p=C.gather_emb_dropout_p,\n",
    "            gather_emb_hidden_dim=C.gather_emb_hidden_dim,\n",
    "            hidden_node_features=C.hidden_node_features,\n",
    "            initialization=C.weights_initialization,\n",
    "            message_passes=C.message_passes,\n",
    "            message_size=C.message_size,\n",
    "            n_nodes_largest_graph=C.max_n_nodes,\n",
    "            node_features=C.dim_nodes[1],\n",
    "        )\n",
    "\n",
    "    net = net.to(TORCH_DEVICE, non_blocking=True)\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_i, edges_i, apd_i = dataset[1]\n",
    "nodes_i, edges_i, apd_i = nodes_i.unsqueeze(0), edges_i.unsqueeze(0), apd_i.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_i_0, edges_i_0, apd_i_0 = dataset[2]\n",
    "nodes_i_1, edges_i_1, apd_i_1 = dataset[3]\n",
    "nodes_i, edges_i, apd_i = torch.stack([nodes_i_0 , nodes_i_1]) , torch.stack([edges_i_0 , edges_i_1] ) , torch.stack([apd_i_0 , apd_i_1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13, 8])"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_level_output torch.Size([2, 13, 100])\n"
     ]
    }
   ],
   "source": [
    "model_apd_out = model(nodes_i ,edges_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 833])"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_apd_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Generator import DrugGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "n_samples=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_batch_size = min(batch_size, n_samples)\n",
    "\n",
    "n_generation_batches = int(n_samples/generation_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DrugGeneration(model=model,\n",
    "                                   batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_level_output torch.Size([6, 13, 100])\n",
      "node_level_output torch.Size([6, 13, 100])\n",
      "Generated 6 molecules\n"
     ]
    }
   ],
   "source": [
    "generated_graphs = []\n",
    "generated_action_likehoods = []\n",
    "generated_final_loglikelihood = []\n",
    "generated_termination = []\n",
    "\n",
    "for idx in range(0, n_generation_batches + 1):\n",
    "    # generate one batch of graphs\n",
    "    (graphs, action_likelihoods, final_loglikelihoods,\n",
    "        termination) = generator.sample()\n",
    "\n",
    "    generated_graphs.extend(graphs)\n",
    "    generated_action_likehoods.extend(action_likelihoods)\n",
    "    generated_final_loglikelihood.extend(final_loglikelihoods)\n",
    "    generated_termination.extend(termination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0012, grad_fn=<UnbindBackward0>),\n",
       " tensor(0.0012, grad_fn=<UnbindBackward0>),\n",
       " tensor(0.0012, grad_fn=<UnbindBackward0>),\n",
       " tensor(0.0012, grad_fn=<UnbindBackward0>),\n",
       " tensor(0.0012, grad_fn=<UnbindBackward0>),\n",
       " tensor(0.0012, grad_fn=<UnbindBackward0>)]"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_action_likehoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-6.7083, grad_fn=<UnbindBackward0>),\n",
       " tensor(-6.7370, grad_fn=<UnbindBackward0>),\n",
       " tensor(-6.6884, grad_fn=<UnbindBackward0>),\n",
       " tensor(-6.7370, grad_fn=<UnbindBackward0>),\n",
       " tensor(-6.7280, grad_fn=<UnbindBackward0>),\n",
       " tensor(-6.7078, grad_fn=<UnbindBackward0>)]"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_final_loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0, dtype=torch.int8),\n",
       " tensor(0, dtype=torch.int8),\n",
       " tensor(0, dtype=torch.int8),\n",
       " tensor(0, dtype=torch.int8),\n",
       " tensor(0, dtype=torch.int8),\n",
       " tensor(0, dtype=torch.int8)]"
      ]
     },
     "execution_count": 857,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MolecularGraph.GenerationGraph at 0x162bc0610>"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
